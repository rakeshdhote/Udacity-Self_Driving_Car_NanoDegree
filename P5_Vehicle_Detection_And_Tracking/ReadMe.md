# Self-Driving Car Engineer Nanodegree 
# Vehicle Detection and Tracking
- - - 
[TOC] 
## 1. Project Overview 
The objective of this project is to create an image/video processing pipeline to detect vehicles and track them using traditional image processing techniques. 

A `vehicle` and `non-vehicle` data is read. The dataset consists of 8792 and 8968 `vehicle` and `non-vehicle` images of size `64x64` pixels. Sample images of both the dataset are plotted below. 

<table> 
<tr> 
<td style="text-align: center;"> 
**Sample Vehicle Images**   
</td>  
<td style="text-align: center;"> 
**Sample Non-Vehicle Images**   
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
<img src='images/v1.png' style="width: 300px;"> 
<img src='images/v2.png' style="width: 300px;"> 
<img src='images/v3.png' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/nv1.png' style="width: 300px;"> 
<img src='images/nv2.png' style="width: 300px;"> 
<img src='images/nv3.png' style="width: 300px;"> 
</td> 
</tr> 
</table> 


## 2. Feature Extraction 

To detect and track vehicles, edges, shape, color and size are commonly used as characterizing features. In this section, we dive in details of how to use these features in training the model for detection and tracking.  

While extracting features, I experimented with the `RGB`,  `HSV`,  `HLS` and `YCrCb` color spaces and fit the classifier and tested it on the test images. The `YCrCb` color space performed best among others, and used in further  analysis. 

### 2.1 Histogram of Oriented Gradients (HOG)  

HOG is a commonly used feature to detect objects in computer vision and image processing. The hog features are obtained using the `get_hog_features` function in `P5_utility_functions`. The SkLearn's `hog()` function with parameters `orientation`, `pixels_per_cell` and `cells_per_block` is used to obtain hog feature vector. On experimenting with different values of the above parameters, I finally settled with   

`orientation` = 9,   
`pixels_per_cell` = (8,8) and   
`cells_per_block` = (2,2). 

The HOG features for sample `vehicle` and `not-vehicle` images are presented for `YCrCb` channels in the following figure separately. The HOG features for `vehicle` is well defined with horizontal and vertical edges for the `Y` channel than other two. The HOG features for `not-vehicle` image is unstructured in all the three channels. 

==HOG features for all the `Y`, `Cr` and `Cb` channels are appended in the feature vector before training the classifier.==   

<table> 
<tr> 
<td style="text-align: center;">  
</td> 
<td style="text-align: center;"> 
**Original** 
</td> 
<td style="text-align: center;"> 
**Y** 
</td> 
<td style="text-align: center;"> 
**Cr** 
</td> 
<td style="text-align: center;"> 
**Cb** 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
Vehicle
</td> 
<td style="text-align: center;"> 
<img src='images/hog_v_orig.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/hog_v_Y.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/hog_v_Cr.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/hog_v_Cb.jpg' style="width: 300px;"> 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
Not-Vehicle
</td> 
<td style="text-align: center;"> 
<img src='images/hog_nv_orig.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/hog_nv_Y.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/hog_nv_Cr.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='images/hog_nv_Cb.jpg' style="width: 300px;"> 
</td> 
</tr> 
</table> 

### 2.2 Spatial Features
The spatial features are obtained using the `bin_spatial` function in `P5_utility_functions`. The `spatial_bins` = (16, 16) is used to obtain a feature vector.  


### 2.3 Histogram Features
The histogram features are obtained using the `color_hist` function in `P5_utility_functions`. The `hist_bins` = 16 is used to obtain the feature vector.  

==The HOG, spatial and histogram features are appended together before feeding it to the classifier. ==    

## 3. Training a Classifier

### 3.1 Feature Normalization
The features vector consisting of the HOG, spatial and histogram features have different scales in their respective feature spaces. In order to aid numerical stability and faster convergence, the features are normalized using the SkLearn's `StandardScaler` object. The normalization standardize features by removing the mean and scaling to unit variance. This is implemented in `data_train_test_split` function in the code.  

### 3.2 Training-Test Data
The training and test data is generated by random shuffling and splitting the training:test dataset in 0.75:0.25 ratio. This is implemented in `data_train_test_split` function in the code.  

### 3.3 Training Classifier and Measure Performance
SkLearn's support-vector classifer `LinearSVC` is used to train the model for its simplicity. In particular, I experimented with `C` parameter, which penalizes misclassification as aids minimizing false positives. I finally settled with `C` = 0.01 for model training.  

On training, the classifier accuracy is ~99.2%. This indicates that the classifier does a good job in classifying `vehicle` and `non-vehicle` images. The trained classifier is saved to a pickle file for later use.  

This is implemented in `data_train_test_split` function in the code. 

## 4. Sliding Window Search

The classifier is trained on `64x64` pixels `vehicle` and `not-vehicle` images. The camera mounted on the car hood streams `1280x720` pixel video with various elements such as sky, environment, road, vehicles, etc. In order to utilize the trained classifier to detect vehicles, the image needs to split into small windows and later scaled to `64x64` pixels for prediction. This can be achieved by sliding window technique. 
 
 
The following figure presents the sliding windows used for the project. The image above the horizon (trees and sky) is not processed as it contains no useful information. Similarly, the bottom portion of image (car hood) is not processed. The sliding windows in a small region expedite computation time. Sliding windows of different sizes are utilized for robust vehicle detection due to its near-far location on the road. The sliding window of size `64x64` (red boxes), `96x96` (green boxes) and `128x128` pixels (blue boxes) are used. The later two windows are resized to `64x64` pixels using the `cv2.resize` function in order to utilize the classifier. Experiments are conducted with overlapping windows with 0.5 and 0.75, with later giving better performance.   

This is implemented in `process_pipeline` function in code. 

<img src='images/sliding_windows.png' style="width: 900px;"> 

## 5. Image Processing Pipeline
 
Using the sliding window technique, the classifier predicts whether the window contains vehicle or not. If the vehicle is detected, the heatmap technique is implemented as a means of rejecting false positives, and this demonstrably reduces the number of false positives. The technique works as follows:  

Define heatmap with `0` valued pixels.   
(ii) If the vehicle is detected, increment value of pixels in the cell by `1`    
(iii) Loop step (ii) for all the windows and threshold pixels above certain value  
(iv) Use `cipy.ndimage.measurements.label` function to define cluster in heatmap to be labeled as a possible vehicle  
(v) Determine vehicle bounding box using the label   
(vi) Plot the detected vehicle.  

This is implemented in `process_pipeline` function in code. 

The following images present heatmap and detected vehicles in the test images.

<table> 
<tr> 
<td style="text-align: center;"> 
**Original Image** 
</td> 
<td style="text-align: center;"> 
**HeatMap** 
</td> 
<td style="text-align: center;"> 
**Vehicle Detection** 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
<img src='test_images/test1.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test1_heatmap.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test1_processed.jpg' style="width: 300px;"> 
</td>
<tr> 
<td style="text-align: center;"> 
<img src='test_images/test2.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test2_heatmap.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test2_processed.jpg' style="width: 300px;"> 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
<img src='test_images/test3.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test3_heatmap.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test3_processed.jpg' style="width: 300px;"> 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
<img src='test_images/test4.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test4_heatmap.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test4_processed.jpg' style="width: 300px;"> 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
<img src='test_images/test5.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test5_heatmap.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test5_processed.jpg' style="width: 300px;"> 
</td> 
</tr> 
<tr> 
<td style="text-align: center;"> 
<img src='test_images/test6.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test6_heatmap.jpg' style="width: 300px;"> 
</td> 
<td style="text-align: center;"> 
<img src='output_images/test6_processed.jpg' style="width: 300px;"> 
</td> 
</tr> 
</table> 

## 6. Video Processing Pipeline

In order to process a video, the image-processing pipeline developed in the earlier section is utilized. Heatmaps use overlapping bounding boxes  to detect vehicles in the image. The pipeline to use heatmap and sample images are presented in the above section. In particular, false positives are removed from the video by thresholding the heatmap.  
 
 
The following video presents the classifier detecting and tracking vehicles. The video contains few false positives and wobbly bounding boxes, but does a good job of detecting and tracking vehicles. 
Click on the image to run the video.  
 

[![Track-2](images/vehicledetandtrack.png)](https://youtu.be/VWZjQgi9e2M)

## 7. Conclusions 

An image/video processing pipeline is built to detect vehicles and track them using image processing techniques. The pipeline detects the vehicle and tracks them successfully. Although, there are few false positives and wobbly bounding boxes, the pipeline does remarkably well under different test images and video. 

Following are the opportunities to build a more robust pipeline for videos: 
* Integrate project 4 and 5 to detect road lines, vehicles and tracking them simultaneously. 
* Keep track of vehicle detection from previous frame and smoothen the tracking using averaging and using a first-order filter. 
* Speed up computations by refactoring the code and use tricks such as calculating HOG features for image once and utilizing it for sliding window search. 

The classifier is trained using vehicle images from the rear. The classifier is not robust to predict vehicles from side or using front images. The developed pipeline may fail under complex combinations of light/environmental/road conditions such as dawm/twilight/night time, rain/snow, road elevation or during taking sharp turns, etc.  
 

- - - 
## 7. Reflections

This was an interesting project to design a robust pipeline for vehicle detection and tracking. The lessons gave me the opportunity to experiment with various image processing techniques, manipulating color spaces and optimizing pipeline parameters. The developed pipeline along with deep learning will be useful in building a robust self-driving car project.  